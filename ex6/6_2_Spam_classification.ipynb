{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Spam Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"You will be training a classifier to classify whether a given email, x, is spam (y = 1) or non-spam (y = 0).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "import re\n",
    "import string\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Preprocessing Emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to process the emails, using regexp normalisation, then map the contents of the email into a word indices vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = open('data/emailSample1.txt')\n",
    "file_contents = file.read()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"> Anyone knows how much it costs to host a web portal ?\\n>\\nWell, it depends on how many visitors you're expecting.\\nThis can be anywhere from less than 10 bucks a month to a couple of $100. \\nYou should checkout http://www.rackspace.com/ or perhaps Amazon EC2 \\nif youre running something big..\\n\\nTo unsubscribe yourself from this mailing list, send an email to:\\ngroupname-unsubscribe@egroups.com\\n\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of the kind of data we are working with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exercise provides a vocabulary list that contains the 1899 most common words in the email spam corpus. Each word is associated with a number, which we will use for our word indices. The list is a .txt file formatted like so:\n",
    "\n",
    "\n",
    "```\n",
    "1\taa\n",
    "2\tab\n",
    "3\tabil\n",
    "...\n",
    "1897\tzdnet\n",
    "1898\tzero\n",
    "1899\tzip\n",
    "```\n",
    "\n",
    "i.e. newlines separating entries, tabs separating indices from words.\n",
    "\n",
    "I have written `get_vocab_dict()` to load the file and convert it into a python dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vocab_dict():\n",
    "    '''\n",
    "    Loads the provided vocab.txt, converts it into a python\n",
    "    dictionary, and returns the dictionary.\n",
    "    '''\n",
    "    \n",
    "    vocab_dict = {}\n",
    "    \n",
    "    file = open('data/vocab.txt')\n",
    "    file_contents = file.readlines()\n",
    "    file.close()\n",
    "\n",
    "    for line in file_contents:\n",
    "        index = line.split()[0]\n",
    "        word = line.split()[1]\n",
    "        \n",
    "        # We're going to use this to look up indices based on words,\n",
    "        # so make words the keys and indices the values\n",
    "        vocab_dict[word] = index\n",
    "    \n",
    "    return vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_email(email_contents):\n",
    "    \n",
    "    '''\n",
    "    Preprocesses the body of an email (email_contents) and returns\n",
    "    a list of indices of the words contained in the email.\n",
    "    '''\n",
    "\n",
    "    word_indices = []\n",
    "    vocab_dict = get_vocab_dict()\n",
    "\n",
    "    \n",
    "    ## Preprocess email    \n",
    "    # Make lower case\n",
    "    email_contents = email_contents.lower()\n",
    "    \n",
    "    # Strip HTML\n",
    "    # Find expressions that start with <, end with >, and do not\n",
    "    # contain either < or > in the middle. Replace them with\n",
    "    # a blank space.\n",
    "    # Regex cheatsheet: https://www.debuggex.com/cheatsheet/regex/python\n",
    "    email_contents = re.sub(r'<[^<>]+>', ' ', email_contents)\n",
    "    \n",
    "    # Numbers\n",
    "    # Replace numbers with the word 'number'\n",
    "    email_contents = re.sub(r'[0-9]+', 'number', email_contents)\n",
    "\n",
    "    # URLs\n",
    "    # Look for strings starting with http:// or https://\n",
    "    # replace with 'httpaddr'\n",
    "    email_contents = re.sub(r'(http|https)://[^\\s]*', 'httpaddr', email_contents)\n",
    "    \n",
    "    # Email addresses\n",
    "    # Look for strings with @ in the middle, replace with 'emailaddr'\n",
    "    email_contents = re.sub(r'[^\\s]+@[^\\s]+', 'emailaddr', email_contents)\n",
    "    \n",
    "    # $ sign\n",
    "    # Replace with 'dollar'\n",
    "    email_contents = re.sub(r'[$]+', 'dollar', email_contents)\n",
    "\n",
    "\n",
    "    ## Create list of words in email\n",
    "    words = email_contents.split()\n",
    "    \n",
    "    for word in words:\n",
    "        # Remove punctuation\n",
    "        word = word.translate(str.maketrans('','',string.punctuation))\n",
    "    \n",
    "        # Remove non-alphanumeric characters\n",
    "        word = re.sub(r'[^a-zA-Z0-9]', '', word)\n",
    "    \n",
    "        # Stem word\n",
    "        word = ps.stem(word)\n",
    "        \n",
    "        # Skip spaces, blank lines\n",
    "        if len(word) < 1:\n",
    "            continue\n",
    "            \n",
    "        # Look up the word in the dictionary and\n",
    "        # add to word_indices if found\n",
    "        if word in vocab_dict:\n",
    "            word_indices.append(int(vocab_dict[word]))\n",
    "        \n",
    "    return word_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected output:\n",
      "86 916 794 1077 883 370 1699 790 1822 1831 ...\n",
      "\n",
      "word_indices:\n",
      "[86, 916, 794, 1077, 883, 370, 1699, 790, 1822, 1831]\n"
     ]
    }
   ],
   "source": [
    "word_indices = process_email(file_contents)\n",
    "\n",
    "# Check our result using Fig. 11 in ex6.pdf\n",
    "print('Expected output:')\n",
    "print('86 916 794 1077 883 370 1699 790 1822 1831 ...\\n')\n",
    "print('word_indices:')\n",
    "print(word_indices[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.2 Extracting Features from Emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert each email into a vector of features x in R^n, where n = 1899 is the number of words in our vocabulary list.\n",
    "\n",
    "\"x_i âˆˆ {0, 1} for an email corresponds to whether the i-th word in the dictionary occurs in the email. That is, x_i = 1 if the i-th word is in the email and x_i = 0 if the i-th word is not present in the email.\"\n",
    "\n",
    "NB - unlike Matlab/Octave (which this course was designed for), in Python our feature vector will run from 0-1898 rather than 1-1899, so we'll be off by one which we'll have to compensate for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def email_features(word_indices):\n",
    "    \n",
    "    '''\n",
    "    Takes in a word_indices vector and \n",
    "    produces a feature vector from the word indices. \n",
    "    '''\n",
    "    \n",
    "    # Total number of words in the vocabulary list\n",
    "    n = 1899\n",
    "    \n",
    "    # Feature vector\n",
    "    x = np.zeros([n, 1])\n",
    "    \n",
    "    for index in word_indices:\n",
    "        x[index - 1] = 1 # -1 because python lists start at 0\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = email_features(word_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of feature vector: 1899\n",
      "Number of non-zero entries: [44]\n"
     ]
    }
   ],
   "source": [
    "print('Length of feature vector:', len(features))\n",
    "print('Number of non-zero entries:', sum(features > 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"You should see that the feature vector had length 1899 and 45 non-zero entries.\"\n",
    "\n",
    "We have 44 non-zero entries. I'm guessing this is due to me using a slightly different stemmer which has stemmed one word differently than expected by the exercise, and that stemmed word doesn't match the provided vocabulary list. Shouldn't be a major problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Training SVM for Spam Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# spamTrain.mat contains 4000 training examples of spam and non-spam emails that\n",
    "# have already been converted into feature vectors like I did above\n",
    "emails_train = scipy.io.loadmat('data/spamTrain.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__header__', '__version__', '__globals__', 'X', 'y'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 1899)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails_train['X'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4000 feature vectors, each of length 1899"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails_train['y'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=0.1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the SVM\n",
    "X = emails_train['X']\n",
    "y = emails_train['y']\n",
    "y = y.flatten()\n",
    "\n",
    "model = svm.SVC(kernel='linear', C=0.1)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 99.825\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(X)\n",
    "print('Training accuracy:', np.mean(pred == y) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"...you should see that the classifier gets a training accuracy of about 99.8%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__header__', '__version__', '__globals__', 'Xtest', 'ytest'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate model on test set\n",
    "\n",
    "# spamTest.mat contains 1000 test examples of spam and non-spam emails that\n",
    "# have already been converted into feature vectors like I did above\n",
    "\n",
    "emails_test = scipy.io.loadmat('data/spamTest.mat')\n",
    "emails_test.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtest = emails_test['Xtest']\n",
    "ytest = emails_test['ytest']\n",
    "ytest = ytest.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 98.9\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(Xtest)\n",
    "print('Test accuracy:', np.mean(pred == ytest) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"you should see ... a test accuracy of about 98.5%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Top Predictors for Spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the words with the largest positive values in the classifier - these are the top predictors of spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1899)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This gives us the weights for each of the 1899 features\n",
    "model.coef_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The position of each element in `model.coef_` corresponds to the feature index, and we need to preserve this information since feature indices correspond to words in our vocaculary list. Easy way to do this is usung `np.argsort()`, which returns the indices that would sort an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices = np.argsort(-model.coef_) # - sign to sort by descending\n",
    "indices = indices.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Our vocab_dict has words as keys and indices as values\n",
    "# which is the opposite of what we now need. So create an\n",
    "# inverse dictionary\n",
    "\n",
    "vocab_dict = get_vocab_dict()\n",
    "index_dict = {v: k for k, v in vocab_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otherwis\n",
      "clearli\n",
      "remot\n",
      "gt\n",
      "visa\n",
      "base\n",
      "doesn\n",
      "wife\n",
      "previous\n",
      "player\n",
      "mortgag\n",
      "natur\n",
      "ll\n",
      "futur\n",
      "hot\n"
     ]
    }
   ],
   "source": [
    "# Print top 15 words\n",
    "for i in range(15):\n",
    "    print(index_dict[str(indices[i])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are very different to the \"most spammy\" words in ex6.pdf Fig. 12. I did a few tests and I'm confident in my code, so I presume this is due to the fact that I'm using a different SVM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
